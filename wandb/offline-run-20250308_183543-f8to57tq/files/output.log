step 0: val loss 10.9780
iter 0: loss 10.9752, time 124933.69ms, lr 5.9880239520958085e-06, norm:  11.2571, tok/sec: 4196.530060820035
iter 1: loss 10.7962, time 6055.70ms, lr 1.1976047904191617e-05, norm:  10.7292, tok/sec: 86577.55934779716
iter 2: loss 10.4976, time 6042.07ms, lr 1.7964071856287426e-05, norm:  9.5490, tok/sec: 86772.87078444766
iter 3: loss 10.2389, time 6038.53ms, lr 2.3952095808383234e-05, norm:  6.8626, tok/sec: 86823.76464699248
iter 4: loss 9.9581, time 6037.18ms, lr 2.994011976047904e-05, norm:  4.5774, tok/sec: 86843.18204713319
iter 5: loss 9.8428, time 6035.09ms, lr 3.592814371257485e-05, norm:  3.4018, tok/sec: 86873.23904501127
iter 6: loss 9.6993, time 6033.20ms, lr 4.1916167664670664e-05, norm:  2.6506, tok/sec: 86900.47675223435
iter 7: loss 9.5631, time 6032.21ms, lr 4.790419161676647e-05, norm:  2.2968, tok/sec: 86914.68599791154
iter 8: loss 9.6823, time 6030.29ms, lr 5.389221556886227e-05, norm:  2.0720, tok/sec: 86942.36563175771
iter 9: loss 9.5890, time 6029.18ms, lr 5.988023952095808e-05, norm:  2.0369, tok/sec: 86958.40761075345
iter 10: loss 9.4873, time 6027.44ms, lr 6.58682634730539e-05, norm:  1.9867, tok/sec: 86983.58268700878
iter 11: loss 9.4994, time 6025.54ms, lr 7.18562874251497e-05, norm:  1.9046, tok/sec: 87011.02391313836
iter 12: loss 9.4246, time 6023.59ms, lr 7.78443113772455e-05, norm:  1.8998, tok/sec: 87039.08189788314
iter 13: loss 9.3621, time 6022.51ms, lr 8.383233532934133e-05, norm:  1.8726, tok/sec: 87054.68744360458
iter 14: loss 9.3066, time 6019.94ms, lr 8.982035928143712e-05, norm:  1.8411, tok/sec: 87091.9097284278
iter 15: loss 9.2788, time 6020.48ms, lr 9.580838323353294e-05, norm:  1.7721, tok/sec: 87084.03233345631
iter 16: loss 9.1474, time 6018.30ms, lr 0.00010179640718562875, norm:  1.7501, tok/sec: 87115.68506083613
iter 17: loss 9.0799, time 6017.01ms, lr 0.00010778443113772454, norm:  1.7479, tok/sec: 87134.36664929545
iter 18: loss 9.0068, time 6017.26ms, lr 0.00011377245508982037, norm:  1.6888, tok/sec: 87130.6517855374
iter 19: loss 8.9106, time 6016.35ms, lr 0.00011976047904191617, norm:  1.6372, tok/sec: 87143.88311699634
iter 20: loss 8.7987, time 6015.84ms, lr 0.00012574850299401196, norm:  1.5964, tok/sec: 87151.2670626624
iter 21: loss 8.6766, time 6016.79ms, lr 0.0001317365269461078, norm:  1.5438, tok/sec: 87137.52936658224
iter 22: loss 8.5871, time 6018.43ms, lr 0.0001377245508982036, norm:  1.5126, tok/sec: 87113.71795403851
iter 23: loss 8.5322, time 6020.16ms, lr 0.0001437125748502994, norm:  1.5017, tok/sec: 87088.64685166781
iter 24: loss 8.5250, time 6017.90ms, lr 0.0001497005988023952, norm:  1.4374, tok/sec: 87121.39362455632
iter 25: loss 8.3927, time 6017.56ms, lr 0.000155688622754491, norm:  1.2709, tok/sec: 87126.29861690148
iter 26: loss 8.2056, time 6019.68ms, lr 0.00016167664670658683, norm:  1.3049, tok/sec: 87095.70752818184
iter 27: loss 8.1546, time 6017.42ms, lr 0.00016766467065868265, norm:  1.1395, tok/sec: 87128.4250906132
iter 28: loss 8.1431, time 6019.24ms, lr 0.00017365269461077845, norm:  1.0090, tok/sec: 87102.05518084596
iter 29: loss 8.0263, time 6018.65ms, lr 0.00017964071856287425, norm:  0.9898, tok/sec: 87110.53280984114
iter 30: loss 7.8692, time 6020.08ms, lr 0.00018562874251497005, norm:  0.9458, tok/sec: 87089.82297673749
iter 31: loss 7.9438, time 6021.37ms, lr 0.00019161676646706587, norm:  0.8500, tok/sec: 87071.2673578735
iter 32: loss 7.7221, time 6022.21ms, lr 0.0001976047904191617, norm:  0.7349, tok/sec: 87059.00243917029
iter 33: loss 7.7697, time 6022.59ms, lr 0.0002035928143712575, norm:  0.7601, tok/sec: 87053.5088211112
iter 34: loss 7.7396, time 6024.61ms, lr 0.0002095808383233533, norm:  0.5878, tok/sec: 87024.35669050545
iter 35: loss 7.7158, time 6024.44ms, lr 0.0002155688622754491, norm:  0.4964, tok/sec: 87026.9155910375
iter 36: loss 7.6445, time 6025.44ms, lr 0.0002215568862275449, norm:  0.5477, tok/sec: 87012.45616440725
iter 37: loss 7.4903, time 6028.01ms, lr 0.00022754491017964074, norm:  0.4235, tok/sec: 86975.31550968638
iter 38: loss 7.6831, time 6027.12ms, lr 0.00023353293413173654, norm:  0.4487, tok/sec: 86988.13494988464
iter 39: loss 7.5656, time 6029.31ms, lr 0.00023952095808383233, norm:  0.4596, tok/sec: 86956.54731793888
iter 40: loss 7.5906, time 6027.85ms, lr 0.00024550898203592813, norm:  0.4941, tok/sec: 86977.6719939367
iter 41: loss 7.5696, time 6028.12ms, lr 0.00025149700598802393, norm:  0.5195, tok/sec: 86973.77096373988
iter 42: loss 7.6497, time 6031.19ms, lr 0.0002574850299401198, norm:  0.4476, tok/sec: 86929.45658484855
iter 43: loss 7.4792, time 6035.38ms, lr 0.0002634730538922156, norm:  0.6227, tok/sec: 86869.04539244779
iter 44: loss 7.5861, time 6033.89ms, lr 0.0002694610778443114, norm:  0.9224, tok/sec: 86890.54643428716
iter 45: loss 7.4631, time 6031.59ms, lr 0.0002754491017964072, norm:  0.5078, tok/sec: 86923.61509207264
iter 46: loss 7.6149, time 6036.00ms, lr 0.000281437125748503, norm:  0.6227, tok/sec: 86860.17553833187
iter 47: loss 7.8193, time 6035.47ms, lr 0.0002874251497005988, norm:  0.4530, tok/sec: 86867.78600301802
iter 48: loss 7.5065, time 6036.44ms, lr 0.0002934131736526946, norm:  0.5089, tok/sec: 86853.77046841098
iter 49: loss 7.6003, time 6037.19ms, lr 0.0002994011976047904, norm:  0.4127, tok/sec: 86843.03457520719
iter 50: loss 7.4442, time 6036.56ms, lr 0.0003053892215568862, norm:  0.6974, tok/sec: 86852.18221213447
iter 51: loss 7.4990, time 6037.99ms, lr 0.000311377245508982, norm:  0.4671, tok/sec: 86831.56073724467
iter 52: loss 7.3784, time 6035.43ms, lr 0.00031736526946107786, norm:  0.4751, tok/sec: 86868.36250372909
iter 53: loss 7.5407, time 6037.93ms, lr 0.00032335329341317366, norm:  0.3147, tok/sec: 86832.40076895023
/leonardo_work/BOOST_LCustodi/script/training/flex_fa_training_env/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass